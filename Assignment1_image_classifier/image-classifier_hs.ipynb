{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyxmEoX11W5u"
      },
      "source": [
        "# Step 1: Download a dataset and preview images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCZqRJrYBAff",
        "outputId": "631895c6-65c7-4cdf-8970-4e23237aaf8f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "tar: Error opening archive: Failed to open ''./cifar100.tar''\n"
          ]
        }
      ],
      "source": [
        "!tar -xvf './cifar100.tar'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woI6j0f5BTGl"
      },
      "source": [
        "# Step 2: Custom Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnU8jRXoBaX_",
        "outputId": "506e2783-df01-4f8e-d6ec-f3e4af0faf1e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import glob\n",
        "import torch\n",
        "import shutil\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "config = {\n",
        "    \"data_path\":\"./cifar100\",\n",
        "    \"batch_size\":128,\n",
        "}\n",
        "\n",
        "class mydataset(Dataset):\n",
        "    def __init__(self, data_dir, flag, transform):\n",
        "        super(mydataset,self).__init__()\n",
        "        self.root       = data_dir\n",
        "        self.label      = flag\n",
        "        self.transform  = transform\n",
        "\n",
        "        self.img_dir = os.path.join(self.root, self.label)\n",
        "        self.img_names  = glob.glob(os.path.join(self.img_dir, '*.jpg'))\n",
        "\n",
        "        self.tags = ['apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', 'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', 'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion', 'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse', 'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear', 'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine', 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', 'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout', 'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm']\n",
        "\n",
        "    def RGB2Gradient(self, img:torch.tensor):\n",
        "        \"\"\"\n",
        "        Converts an RGB image tensor to its gradient magnitude using Sobel operator.\n",
        "        The output is replicated to 3 channels to match the input dimensions of the other branch.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Define Sobel kernels\n",
        "        sobel_kernel_x = torch.tensor([[-1., 0., 1.], \n",
        "                                       [-2., 0., 2.], \n",
        "                                       [-1., 0., 1.]], dtype=torch.float32).reshape((1, 1, 3, 3))\n",
        "        sobel_kernel_y = torch.tensor([[-1., -2., -1.], \n",
        "                                       [ 0.,  0.,  0.], \n",
        "                                       [ 1.,  2.,  1.]], dtype=torch.float32).reshape((1, 1, 3, 3))\n",
        "\n",
        "        sobel_kernel_x = sobel_kernel_x.to(img.device)\n",
        "        sobel_kernel_y = sobel_kernel_y.to(img.device)\n",
        "\n",
        "        # Convert to grayscale: [3, H, W] -> Output [1, H, W]\n",
        "        # gray_img = img[0, :, :] * 0.2989 + img[1, :, :] * 0.5870 + img[2, :, :] * 0.1140\n",
        "        # gray_img = gray_img.unsqueeze(0)\n",
        "        gray_img = transforms.Grayscale(num_output_channels=1)(img)\n",
        "\n",
        "        # Add batch dimension [1, H, W] -> [1, 1, H, W]\n",
        "        gray_img_batch = gray_img.unsqueeze(0) \n",
        "\n",
        "        # [YOU NEED TO FILL] Apply Sobel filters\n",
        "        # Use F.conv2d, gray_img_batch, and the Sobel kernels (sobel_kernel_x, sobel_kernel_y) \n",
        "        # to calculate the gradients in the x and y directions.\n",
        "        grad_x = F.conv2d(gray_img_batch, sobel_kernel_x, padding=1)\n",
        "        grad_y = F.conv2d(gray_img_batch, sobel_kernel_y, padding=1)\n",
        "        \n",
        "        # [YOU NEED TO FILL] Calculate gradient magnitude ---\n",
        "        # Calculate the magnitude (G = sqrt(Gx^2 + Gy^2)) from grad_x and grad_y.\n",
        "        # The result should be stored in a variable named 'magnitude'.\n",
        "        magnitude = torch.sqrt(grad_x ** 2 + grad_y ** 2)\n",
        "\n",
        "        # Normalize magnitude\n",
        "        mag_min = magnitude.min()\n",
        "        mag_max = magnitude.max()\n",
        "        epsilon = 1e-6 # Avoid division by zero\n",
        "        normalized_magnitude = (magnitude - mag_min) / (mag_max - mag_min + epsilon)\n",
        "        \n",
        "        # Replicate to 3 channels [1, 1, H, W] -> [1, 3, H, W]\n",
        "        normalized_magnitude_3channel = normalized_magnitude.repeat(1, 3, 1, 1)\n",
        "\n",
        "        # Remove batch dimension [1, 3, H, W] -> [3, H, W]\n",
        "        return normalized_magnitude_3channel.squeeze(0)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        img_name = self.img_names[idx]\n",
        "        img = Image.open(os.path.join(img_name)).convert('RGB')\n",
        "        img = self.transform(img)\n",
        "        \n",
        "        # Compute gradient magnitude image\n",
        "        grad_img = self.RGB2Gradient(img)\n",
        "\n",
        "        tag = 0\n",
        "        for i in range(len(self.tags)):\n",
        "            if self.tags[i] in img_name:\n",
        "                tag = i\n",
        "                break\n",
        "\n",
        "        return img, grad_img, tag\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_names)\n",
        "\n",
        "transform_train = transforms.Compose(\n",
        "        [transforms.Resize([72, 72]), # 稍微放大，再随机裁剪\n",
        "         transforms.RandomResizedCrop(64, scale=(0.8, 1.0)), # 随机裁剪拉伸\n",
        "         transforms.RandomHorizontalFlip(p=0.5), # 随机水平翻转\n",
        "         transforms.RandomRotation(degrees=15), # 随机旋转±15度\n",
        "        #  transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1), # 颜色抖动\n",
        "         transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)), # 随机模糊\n",
        "         transforms.ToTensor()])\n",
        "transform_test = transforms.Compose(\n",
        "    [transforms.Resize([64, 64]),\n",
        "      transforms.ToTensor()])\n",
        "\n",
        "train_dataset = mydataset(data_dir=config['data_path'], flag= \"train\", transform=transform_train)\n",
        "test_dataset  = mydataset(data_dir=config['data_path'], flag= \"test\", transform=transform_test)\n",
        "\n",
        "# define data loader\n",
        "train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config['batch_size'], shuffle=True, num_workers=0, pin_memory=True, drop_last=False)\n",
        "\n",
        "test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=config['batch_size'], shuffle=True, num_workers=0, pin_memory=True, drop_last=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0io-L6VCDoB"
      },
      "source": [
        "# Step 3: Configure the Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {
        "id": "9Jvia9QHCHtC"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.nn import Sequential as Seq\n",
        "\n",
        "class CommonBlock(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel, stride):        # 普通Block简单完成两次卷积操作\n",
        "        super(CommonBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channel)\n",
        "        self.conv2 = nn.Conv2d(out_channel, out_channel, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channel)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x                                            # 普通Block的shortcut为直连，不需要升维下采样\n",
        "\n",
        "        x = F.relu(self.bn1(self.conv1(x)), inplace=True)       # 完成一次卷积\n",
        "        x = self.bn2(self.conv2(x))                             # 第二次卷积不加relu激活函数\n",
        "\n",
        "        x += identity                                           # 两路相加\n",
        "        return F.relu(x, inplace=True)                          # 添加激活函数输出\n",
        "\n",
        "class SpecialBlock(nn.Module):                                  # 特殊Block完成两次卷积操作，以及一次升维下采样\n",
        "    def __init__(self, in_channel, out_channel, stride):        # 注意这里的stride传入一个数组，shortcut和残差部分stride不同\n",
        "        super(SpecialBlock, self).__init__()\n",
        "        self.change_channel = nn.Sequential(                    # 负责升维下采样的卷积网络change_channel\n",
        "            nn.Conv2d(in_channel, out_channel, kernel_size=1, stride=stride[0], padding=0, bias=False),\n",
        "            nn.BatchNorm2d(out_channel)\n",
        "        )\n",
        "        self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=3, stride=stride[0], padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channel)\n",
        "        self.conv2 = nn.Conv2d(out_channel, out_channel, kernel_size=3, stride=stride[1], padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channel)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = self.change_channel(x)                       # 调用change_channel对输入修改，为后面相加做变换准备\n",
        "\n",
        "        x = F.relu(self.bn1(self.conv1(x)), inplace=True)\n",
        "        x = self.bn2(self.conv2(x))                             # 完成残差部分的卷积\n",
        "\n",
        "        x += identity\n",
        "        return F.relu(x, inplace=True)                          # 输出卷积单元\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    # 修改为单分支特征提取网络，可被复用为双分支网络的子模块\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.prepare = nn.Sequential(           # 预处理==》[batch, 64, 56, 56]\n",
        "            nn.Conv2d(3, 64, 7, 2, 3),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(3, 2, 1)\n",
        "        )\n",
        "        self.layer1 = nn.Sequential(            # layer1有点特别，由于输入输出的channel均是64，故两个CommonBlock\n",
        "            CommonBlock(64, 64, 1),\n",
        "            CommonBlock(64, 64, 1)\n",
        "        )\n",
        "        self.layer2 = nn.Sequential(            # layer234类似，由于输入输出的channel不同，故一个SpecialBlock，一个CommonBlock\n",
        "            SpecialBlock(64, 128, [2, 1]),\n",
        "            CommonBlock(128, 128, 1)\n",
        "        )\n",
        "        self.layer3 = nn.Sequential(\n",
        "            SpecialBlock(128, 256, [2, 1]),\n",
        "            CommonBlock(256, 256, 1)\n",
        "        )\n",
        "        self.layer4 = nn.Sequential(\n",
        "            SpecialBlock(256, 512, [2, 1]),\n",
        "            CommonBlock(512, 512, 1)\n",
        "        )\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d(1)  # 全局平均池化\n",
        "    def forward(self, x):\n",
        "        x = self.prepare(x)         # 预处理\n",
        "\n",
        "        x = self.layer1(x)          # 四个卷积单元\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.global_pool(x)    # 全局平均池化\n",
        "        x = torch.flatten(x, 1)    # 展平\n",
        "\n",
        "        return x\n",
        "\n",
        "# ------------------- 双分支模型定义 -------------------\n",
        "class DualBranchNet(nn.Module):\n",
        "    def __init__(self, classes_num=100):\n",
        "        super(DualBranchNet, self).__init__()\n",
        "        self.rgb_net = ConvNet()\n",
        "        self.grad_net = ConvNet()\n",
        "        \n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, classes_num)\n",
        "        )\n",
        "        \n",
        "    def forward(self, rgb, grad):\n",
        "        rgb_feat = self.rgb_net(rgb)\n",
        "        grad_feat = self.grad_net(grad)\n",
        "        \n",
        "        fused = torch.cat([rgb_feat, grad_feat], dim=1)\n",
        "        out = self.classifier(fused)\n",
        "        return out\n",
        "    \n",
        "\n",
        "config = {\n",
        "    \"lr\":1e-2,\n",
        "    \"momentum\":0.9,\n",
        "    \"weight_decay\":1e-4,\n",
        "}\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "net = DualBranchNet(classes_num=100).to(device)\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=config[\"momentum\"], weight_decay=config[\"weight_decay\"])\n",
        "# optimizer = torch.optim.AdamW(net.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYKk56zLCxbk"
      },
      "source": [
        "# Step 4: Train the network and save model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAfQYydqC_R7",
        "outputId": "047cbac2-da53-48ac-92e7-8a72a86decfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch[1]:[001/391] Time:0.2816 Data:0.2071  loss:4.6810(4.6810)  prec@1:1.56(1.56)  \n",
            "Epoch[1]:[051/391] Time:0.2151 Data:0.2001  loss:4.1628(4.3578)  prec@1:10.16(8.75)  \n",
            "Epoch[1]:[101/391] Time:0.2124 Data:0.2019  loss:4.0259(4.1863)  prec@1:6.25(8.86)  \n",
            "Epoch[1]:[151/391] Time:0.4277 Data:0.3990  loss:3.6777(4.0549)  prec@1:9.38(9.35)  \n",
            "Epoch[1]:[201/391] Time:0.4124 Data:0.3808  loss:3.5198(3.9562)  prec@1:14.06(10.15)  \n",
            "Epoch[1]:[251/391] Time:0.3876 Data:0.3570  loss:3.6440(3.8773)  prec@1:14.06(10.66)  \n",
            "Epoch[1]:[301/391] Time:0.4102 Data:0.3871  loss:3.3660(3.8068)  prec@1:10.94(11.37)  \n",
            "Epoch[1]:[351/391] Time:0.4330 Data:0.4099  loss:3.2942(3.7477)  prec@1:17.97(12.09)  \n",
            "Epoch time: 136s\n",
            "Saving models......\n",
            "Epoch[2]:[001/391] Time:0.4097 Data:0.3876  loss:2.9193(2.9193)  prec@1:24.22(24.22)  \n",
            "Epoch[2]:[051/391] Time:0.3646 Data:0.3410  loss:3.0499(3.2470)  prec@1:21.09(19.38)  \n",
            "Epoch[2]:[101/391] Time:0.4046 Data:0.3780  loss:2.9457(3.2199)  prec@1:23.44(19.53)  \n",
            "Epoch[2]:[151/391] Time:0.3743 Data:0.3507  loss:3.1347(3.1939)  prec@1:18.75(19.75)  \n",
            "Epoch[2]:[201/391] Time:0.4182 Data:0.3911  loss:3.2177(3.1736)  prec@1:16.41(19.88)  \n",
            "Epoch[2]:[251/391] Time:0.3945 Data:0.3690  loss:3.1219(3.1583)  prec@1:25.00(20.28)  \n",
            "Epoch[2]:[301/391] Time:0.3920 Data:0.3674  loss:2.7441(3.1299)  prec@1:31.25(20.95)  \n",
            "Epoch[2]:[351/391] Time:0.4212 Data:0.3851  loss:2.8541(3.1005)  prec@1:25.78(21.43)  \n",
            "Epoch time: 153s\n",
            "Saving models......\n",
            "Epoch[3]:[001/391] Time:0.4004 Data:0.3767  loss:3.0664(3.0664)  prec@1:18.75(18.75)  \n",
            "Epoch[3]:[051/391] Time:0.4044 Data:0.3791  loss:2.6843(2.8433)  prec@1:32.81(25.90)  \n",
            "Epoch[3]:[101/391] Time:0.3972 Data:0.3746  loss:2.5846(2.8127)  prec@1:25.78(26.92)  \n",
            "Epoch[3]:[151/391] Time:0.4275 Data:0.4029  loss:2.5772(2.7999)  prec@1:30.47(27.18)  \n",
            "Epoch[3]:[201/391] Time:0.3838 Data:0.3619  loss:2.7916(2.7958)  prec@1:26.56(27.25)  \n",
            "Epoch[3]:[251/391] Time:0.3986 Data:0.3730  loss:2.8445(2.7809)  prec@1:21.88(27.60)  \n",
            "Epoch[3]:[301/391] Time:0.4111 Data:0.3871  loss:2.6961(2.7731)  prec@1:31.25(27.89)  \n",
            "Epoch[3]:[351/391] Time:0.4061 Data:0.3790  loss:2.6769(2.7625)  prec@1:26.56(28.08)  \n",
            "Epoch time: 154s\n",
            "Saving models......\n",
            "Epoch[4]:[001/391] Time:0.4170 Data:0.3874  loss:2.6238(2.6238)  prec@1:30.47(30.47)  \n",
            "Epoch[4]:[051/391] Time:0.4076 Data:0.3737  loss:2.5308(2.5741)  prec@1:31.25(31.43)  \n",
            "Epoch[4]:[101/391] Time:0.3965 Data:0.3725  loss:2.6133(2.5765)  prec@1:33.59(31.93)  \n",
            "Epoch[4]:[151/391] Time:0.3833 Data:0.3593  loss:2.7260(2.5781)  prec@1:34.38(32.12)  \n",
            "Epoch[4]:[201/391] Time:0.4164 Data:0.3892  loss:2.7647(2.5730)  prec@1:32.03(32.21)  \n",
            "Epoch[4]:[251/391] Time:0.3933 Data:0.3707  loss:2.2915(2.5660)  prec@1:39.84(32.20)  \n",
            "Epoch[4]:[301/391] Time:0.3925 Data:0.3695  loss:2.7502(2.5603)  prec@1:28.91(32.37)  \n",
            "Epoch[4]:[351/391] Time:0.4060 Data:0.3809  loss:2.6470(2.5576)  prec@1:35.16(32.47)  \n",
            "Epoch time: 154s\n",
            "Saving models......\n",
            "Epoch[5]:[001/391] Time:0.4097 Data:0.3846  loss:2.6652(2.6652)  prec@1:38.28(38.28)  \n",
            "Epoch[5]:[051/391] Time:0.4020 Data:0.3854  loss:2.3682(2.4457)  prec@1:42.19(35.20)  \n",
            "Epoch[5]:[101/391] Time:0.3903 Data:0.3567  loss:2.4223(2.4110)  prec@1:37.50(35.88)  \n",
            "Epoch[5]:[151/391] Time:0.3860 Data:0.3584  loss:2.2566(2.4133)  prec@1:39.06(35.63)  \n",
            "Epoch[5]:[201/391] Time:0.3930 Data:0.3669  loss:2.5228(2.4055)  prec@1:31.25(35.89)  \n",
            "Epoch[5]:[251/391] Time:0.4014 Data:0.3765  loss:2.1592(2.3982)  prec@1:42.97(36.05)  \n",
            "Epoch[5]:[301/391] Time:0.4017 Data:0.3758  loss:2.4574(2.4014)  prec@1:32.03(36.08)  \n",
            "Epoch[5]:[351/391] Time:0.3886 Data:0.3660  loss:2.5133(2.4005)  prec@1:28.12(36.13)  \n",
            "Epoch time: 154s\n",
            "Saving models......\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "class AverageMeter(object):\n",
        "    def __init__(self):\n",
        "      self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "      self.val   = 0\n",
        "      self.avg   = 0\n",
        "      self.sum   = 0\n",
        "      self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "      self.val   = val\n",
        "      self.sum   += val * n\n",
        "      self.count += n\n",
        "      self.avg   = self.sum / self.count\n",
        "\n",
        "def accuracy(output, target, topk=(1,1)):\n",
        "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred    = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "      correct_k = correct[:k].view(-1).float().sum(0)\n",
        "      res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res[0]\n",
        "\n",
        "def train(train_loader, net, optimizer, criterion, epoch):\n",
        "    batch_time = AverageMeter()\n",
        "    data_time  = AverageMeter()\n",
        "    top1       = AverageMeter()\n",
        "\n",
        "    LOSS = AverageMeter()\n",
        "\n",
        "    net.train()\n",
        "\n",
        "    end = time.time()\n",
        "    for i, (rgb, grad, target) in enumerate(train_loader, start=1):\n",
        "        data_time.update(time.time() - end)\n",
        "\n",
        "        rgb = rgb.to(device)\n",
        "        grad = grad.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        out = net(rgb, grad)\n",
        "\n",
        "        loss = criterion(out, target)\n",
        "\n",
        "        prec1 = accuracy(out, target) # prec1:list\n",
        "        top1.update(prec1.item(), rgb.size(0))\n",
        "\n",
        "        LOSS.update(loss.item(), rgb.size(0))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        if i % 50 == 1:\n",
        "            log_str = ('Epoch[{0}]:[{1:03}/{2:03}] '\n",
        "                       'Time:{batch_time.val:.4f} '\n",
        "                       'Data:{data_time.val:.4f}  '\n",
        "                       'loss:{loss.val:.4f}({loss.avg:.4f})  '\n",
        "                       'prec@1:{top1.val:.2f}({top1.avg:.2f})  '.format(\n",
        "                       epoch, i, len(train_loader), batch_time=batch_time, data_time=data_time,\n",
        "                       loss=LOSS,\n",
        "                       top1=top1))\n",
        "            print(log_str)\n",
        "\n",
        "    return LOSS.avg\n",
        "\n",
        "def save_checkpoint(state, is_best, save_root, epoch):\n",
        "    if not os.path.exists(save_root):\n",
        "        os.makedirs(save_root)\n",
        "    save_path = os.path.join(save_root, 'epoch_{}.pth.tar'.format(str(epoch)))\n",
        "    torch.save(state, save_path)\n",
        "    if is_best:\n",
        "        best_save_path = os.path.join(save_root, 'model_best.pth.tar')\n",
        "        shutil.copyfile(save_path, best_save_path)\n",
        "\n",
        "config = {\n",
        "    \"save_root\":\"./result\",\n",
        "    \"epochs\":5,\n",
        "}\n",
        "\n",
        "best_top1 = 0\n",
        "test_top1 = 0\n",
        "for epoch in range(1, config[\"epochs\"]+1):\n",
        "    # train one epoch\n",
        "    epoch_start_time = time.time()\n",
        "    train_loss = train(train_loader, net, optimizer, criterion, epoch)\n",
        "\n",
        "    # evaluate on testing set\n",
        "    # test_top1 = test(test_loader, net, criterion)\n",
        "\n",
        "    epoch_duration = time.time() - epoch_start_time\n",
        "    print('Epoch time: {}s'.format(int(epoch_duration)))\n",
        "\n",
        "    # save model\n",
        "    is_best = False\n",
        "    # if test_top1 > best_top1:\n",
        "    #     best_top1 = test_top1\n",
        "    #     is_best = True\n",
        "    print('Saving models......')\n",
        "    save_checkpoint({\n",
        "        'epoch': epoch,\n",
        "        'net': net.state_dict(),\n",
        "        'prec@1': test_top1,\n",
        "    }, is_best, config[\"save_root\"], epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WO1MiuPHFpVW"
      },
      "source": [
        "# Step 5: Test on single image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFsBN0Z0FxkT",
        "outputId": "c498d1e4-0e97-4e91-8317-a695211475c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "apple\n"
          ]
        }
      ],
      "source": [
        "img = Image.open(\"./cifar100/test/apple_9904.jpg\")\n",
        "img = transform_test(img)\n",
        "grad = test_dataset.RGB2Gradient(img)\n",
        "img = img.to(device).unsqueeze(0)\n",
        "grad = grad.to(device).unsqueeze(0)\n",
        "out = net(img, grad)\n",
        "predicted_classes = torch.argmax(out, dim=1)\n",
        "tags = ['apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', 'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', 'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion', 'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse', 'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear', 'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine', 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', 'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout', 'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm']\n",
        "\n",
        "print(tags[predicted_classes[0]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ne7evEhmEI0t"
      },
      "source": [
        "# Step 6: Evaluate model accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgH66ll9EV0p",
        "outputId": "d9579d0f-155a-4068-ba05-5c4b0e6e97ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------test classification result---------------------------------\n",
            "Loss: 3.0638, Prec@1: 35.33%\n"
          ]
        }
      ],
      "source": [
        "def test(test_loader, net, criterion):\n",
        "    losses = AverageMeter()\n",
        "    top1   = AverageMeter()\n",
        "\n",
        "    net.eval()\n",
        "\n",
        "    for i, (rgb, grad, target) in enumerate(test_loader, start=1):\n",
        "        rgb = rgb.to(device)\n",
        "        grad = grad.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out = net(rgb, grad)\n",
        "            loss = criterion(out, target)\n",
        "\n",
        "        prec1 = accuracy(out, target) # prec1:list\n",
        "        losses.update(loss.item(), img.size(0))\n",
        "        top1.update(prec1.item(), img.size(0))\n",
        "\n",
        "    f_l = [losses.avg, top1.avg]\n",
        "    print('---------------------------------test classification result---------------------------------')\n",
        "    print('Loss: {:.4f}, Prec@1: {:.2f}%'.format(*f_l))\n",
        "\n",
        "    return top1.avg\n",
        "\n",
        "test_top1 = test(test_loader, net, criterion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0v4gr3XWJDBQ"
      },
      "source": [
        "# Step 7: T-SNE Visualization\n",
        "\n",
        "## Use hooks in PyTorch to extract feature representations from the intermediate layers of the model for the test set \"testloader\", and visualize them using the T-SNE method. The specific requirements are as follows:\n",
        "#### Visualize the features before and after the dual-branch feature fusion. If there are multiple fusions, you may choose specific layers for visualization.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
