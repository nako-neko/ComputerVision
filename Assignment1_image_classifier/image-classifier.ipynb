{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2uffHZLCPMU",
        "outputId": "a2bf1a9b-946a-4b8c-f5f1-f068fe45cc1f"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyxmEoX11W5u"
      },
      "source": [
        "# Step 1: Download a dataset and preview images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCZqRJrYBAff",
        "outputId": "631895c6-65c7-4cdf-8970-4e23237aaf8f"
      },
      "outputs": [],
      "source": [
        "# !tar -xvf './cifar100.tar'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woI6j0f5BTGl"
      },
      "source": [
        "# Step 2: Custom Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnU8jRXoBaX_",
        "outputId": "506e2783-df01-4f8e-d6ec-f3e4af0faf1e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import glob\n",
        "import torch\n",
        "import shutil\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "config = {\n",
        "    \"data_path\":\"./cifar100\",\n",
        "    \"batch_size\": 128,\n",
        "}\n",
        "\n",
        "class mydataset(Dataset):\n",
        "    def __init__(self, data_dir, flag, transform):\n",
        "        super(mydataset,self).__init__()\n",
        "        self.root       = data_dir\n",
        "        self.label      = flag\n",
        "        self.transform  = transform\n",
        "\n",
        "        self.img_dir = os.path.join(self.root, self.label)\n",
        "        self.img_names  = glob.glob(os.path.join(self.img_dir, '*.jpg'))\n",
        "\n",
        "        self.tags = ['apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', 'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', 'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion', 'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse', 'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear', 'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine', 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', 'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout', 'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm']\n",
        "\n",
        "    def RGB2Gradient(self, img:torch.tensor):\n",
        "        \"\"\"\n",
        "        Converts an RGB image tensor to its gradient magnitude using Sobel operator.\n",
        "        The output is replicated to 3 channels to match the input dimensions of the other branch.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Define Sobel kernels\n",
        "        sobel_kernel_x = torch.tensor([[-1., 0., 1.], \n",
        "                                       [-2., 0., 2.], \n",
        "                                       [-1., 0., 1.]], dtype=torch.float32).reshape((1, 1, 3, 3))\n",
        "        sobel_kernel_y = torch.tensor([[-1., -2., -1.], \n",
        "                                       [ 0.,  0.,  0.], \n",
        "                                       [ 1.,  2.,  1.]], dtype=torch.float32).reshape((1, 1, 3, 3))\n",
        "\n",
        "        sobel_kernel_x = sobel_kernel_x.to(img.device)\n",
        "        sobel_kernel_y = sobel_kernel_y.to(img.device)\n",
        "\n",
        "        # Convert to grayscale: [3, H, W] -> Output [1, H, W]\n",
        "        # gray_img = img[0, :, :] * 0.2989 + img[1, :, :] * 0.5870 + img[2, :, :] * 0.1140\n",
        "        # gray_img = gray_img.unsqueeze(0)\n",
        "        gray_img = transforms.Grayscale(num_output_channels=1)(img)\n",
        "\n",
        "        # Add batch dimension [1, H, W] -> [1, 1, H, W]\n",
        "        gray_img_batch = gray_img.unsqueeze(0) \n",
        "\n",
        "        # [YOU NEED TO FILL] Apply Sobel filters\n",
        "        # Use F.conv2d, gray_img_batch, and the Sobel kernels (sobel_kernel_x, sobel_kernel_y) \n",
        "        # to calculate the gradients in the x and y directions.\n",
        "        grad_x = F.conv2d(gray_img_batch, sobel_kernel_x, padding=1)\n",
        "        grad_y = F.conv2d(gray_img_batch, sobel_kernel_y, padding=1)\n",
        "        \n",
        "        # [YOU NEED TO FILL] Calculate gradient magnitude ---\n",
        "        # Calculate the magnitude (G = sqrt(Gx^2 + Gy^2)) from grad_x and grad_y.\n",
        "        # The result should be stored in a variable named 'magnitude'.\n",
        "        magnitude = torch.sqrt(grad_x ** 2 + grad_y ** 2)\n",
        "\n",
        "        # Normalize magnitude\n",
        "        mag_min = magnitude.min()\n",
        "        mag_max = magnitude.max()\n",
        "        epsilon = 1e-6 # Avoid division by zero\n",
        "        normalized_magnitude = (magnitude - mag_min) / (mag_max - mag_min + epsilon)\n",
        "        \n",
        "        # Replicate to 3 channels [1, 1, H, W] -> [1, 3, H, W]\n",
        "        normalized_magnitude_3channel = normalized_magnitude.repeat(1, 3, 1, 1)\n",
        "\n",
        "        # Remove batch dimension [1, 3, H, W] -> [3, H, W]\n",
        "        return normalized_magnitude_3channel.squeeze(0)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        img_name = self.img_names[idx]\n",
        "        img = Image.open(os.path.join(img_name)).convert('RGB')\n",
        "        \n",
        "        # original image\n",
        "        img = self.transform(img)\n",
        "        \n",
        "        # gradient image\n",
        "        grad_img = self.RGB2Gradient(img)\n",
        "\n",
        "        for i in range(len(self.tags)):\n",
        "            if self.tags[i] in img_name:\n",
        "                tag = i\n",
        "                break\n",
        "\n",
        "        return img, grad_img, tag\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_names)\n",
        "\n",
        "\n",
        "cifar100_mean = [0.5071, 0.4865, 0.4409]\n",
        "cifar100_std = [0.2673, 0.2564, 0.2762]\n",
        "transform_train = transforms.Compose(\n",
        "        [transforms.Resize([64, 64]),\n",
        "         transforms.RandomHorizontalFlip(),    # 50% 概率随机水平翻转\n",
        "         transforms.RandomCrop(64, padding=4), # 在图像周围填充4个像素，然后随机裁剪回 64x64\n",
        "         transforms.ToTensor(),\n",
        "         transforms.Normalize(mean=cifar100_mean, std=cifar100_std)])\n",
        "transform_test = transforms.Compose(\n",
        "    [transforms.Resize([64, 64]),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize(mean=cifar100_mean, std=cifar100_std)])\n",
        "\n",
        "train_dataset = mydataset(data_dir=config['data_path'], flag= \"train\", transform=transform_train)\n",
        "test_dataset  = mydataset(data_dir=config['data_path'], flag= \"test\", transform=transform_test)\n",
        "\n",
        "# define data loader\n",
        "train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config['batch_size'], shuffle=True, num_workers=0, pin_memory=True, drop_last=False)\n",
        "\n",
        "test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=config['batch_size'], shuffle=True, num_workers=0, pin_memory=True, drop_last=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0io-L6VCDoB"
      },
      "source": [
        "# Step 3: Configure the Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "9Jvia9QHCHtC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.nn import Sequential as Seq\n",
        "\n",
        "class CommonBlock(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel, stride):        # 普通Block简单完成两次卷积操作\n",
        "        super(CommonBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channel)\n",
        "        self.conv2 = nn.Conv2d(out_channel, out_channel, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channel)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x                                            # 普通Block的shortcut为直连，不需要升维下采样\n",
        "\n",
        "        x = F.relu(self.bn1(self.conv1(x)), inplace=True)       # 完成一次卷积\n",
        "        x = self.bn2(self.conv2(x))                             # 第二次卷积不加relu激活函数\n",
        "\n",
        "        x += identity                                           # 两路相加\n",
        "        return F.relu(x, inplace=True)                          # 添加激活函数输出\n",
        "\n",
        "class SpecialBlock(nn.Module):                                  # 特殊Block完成两次卷积操作，以及一次升维下采样\n",
        "    def __init__(self, in_channel, out_channel, stride):        # 注意这里的stride传入一个数组，shortcut和残差部分stride不同\n",
        "        super(SpecialBlock, self).__init__()\n",
        "        self.change_channel = nn.Sequential(                    # 负责升维下采样的卷积网络change_channel\n",
        "            nn.Conv2d(in_channel, out_channel, kernel_size=1, stride=stride[0], padding=0, bias=False),\n",
        "            nn.BatchNorm2d(out_channel)\n",
        "        )\n",
        "        self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=3, stride=stride[0], padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channel)\n",
        "        self.conv2 = nn.Conv2d(out_channel, out_channel, kernel_size=3, stride=stride[1], padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channel)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = self.change_channel(x)                       # 调用change_channel对输入修改，为后面相加做变换准备\n",
        "\n",
        "        x = F.relu(self.bn1(self.conv1(x)), inplace=True)\n",
        "        x = self.bn2(self.conv2(x))                             # 完成残差部分的卷积\n",
        "\n",
        "        x += identity\n",
        "        return F.relu(x, inplace=True)                          # 输出卷积单元\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, classes_num):\n",
        "        super(ConvNet, self).__init__()\n",
        "\n",
        "        # --- RGB 分支 ---\n",
        "        self.prepare_rgb = nn.Sequential(           # 预处理==》[batch, 64, 56, 56]\n",
        "            nn.Conv2d(3, 64, 7, 2, 3),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(3, 2, 1)\n",
        "        )\n",
        "        self.layer1_rgb = nn.Sequential(            # layer1有点特别，由于输入输出的channel均是64，故两个CommonBlock\n",
        "            CommonBlock(64, 64, 1),\n",
        "            CommonBlock(64, 64, 1)\n",
        "        )\n",
        "        self.layer2_rgb = nn.Sequential(            # layer234类似，由于输入输出的channel不同，故一个SpecialBlock，一个CommonBlock\n",
        "            SpecialBlock(64, 128, [2, 1]),\n",
        "            CommonBlock(128, 128, 1)\n",
        "        )\n",
        "        self.layer3_rgb = nn.Sequential(\n",
        "            SpecialBlock(128, 256, [2, 1]),\n",
        "            CommonBlock(256, 256, 1)\n",
        "        )\n",
        "        self.layer4_rgb = nn.Sequential(\n",
        "            SpecialBlock(256, 512, [2, 1]),\n",
        "            CommonBlock(512, 512, 1)\n",
        "        )\n",
        "        \n",
        "        # --- 梯度分支 ---\n",
        "        self.prepare_grad = nn.Sequential(           # 预处理==》[batch, 64, 56, 56]\n",
        "            nn.Conv2d(3, 64, 7, 2, 3),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(3, 2, 1)\n",
        "        )\n",
        "        self.layer1_grad = nn.Sequential(            # layer1有点特别，由于输入输出的channel均是64，故两个CommonBlock\n",
        "            CommonBlock(64, 64, 1),\n",
        "            CommonBlock(64, 64, 1)\n",
        "        )\n",
        "        self.layer2_grad = nn.Sequential(            # layer234类似，由于输入输出的channel不同，故一个SpecialBlock，一个CommonBlock\n",
        "            SpecialBlock(64, 128, [2, 1]),\n",
        "            CommonBlock(128, 128, 1)\n",
        "        )\n",
        "        self.layer3_grad = nn.Sequential(\n",
        "            SpecialBlock(128, 256, [2, 1]),\n",
        "            CommonBlock(256, 256, 1)\n",
        "        )\n",
        "        self.layer4_grad = nn.Sequential(\n",
        "            SpecialBlock(256, 512, [2, 1]),\n",
        "            CommonBlock(512, 512, 1)\n",
        "        )\n",
        "\n",
        "        # --- 融合和分类头 ---\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.flatten = nn.Flatten(start_dim=1)\n",
        "        \n",
        "        # 融合层\n",
        "        self.fc_fused = nn.Linear(512 + 512, 1024)\n",
        "        self.dropout = nn.Dropout(p=0.3) # Dropout 层\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "        # 分类器\n",
        "        self.classifier = nn.Linear(1024, classes_num)\n",
        "        \n",
        "        # self.head = Seq(nn.AdaptiveAvgPool2d(1),\n",
        "        #           nn.Flatten(start_dim=1),\n",
        "        #           nn.Linear(512, classes_num))\n",
        "\n",
        "    def forward(self, x_rgb, x_grad):\n",
        "        # x = self.prepare(x)         # 预处理\n",
        "\n",
        "        # x = self.layer1(x)          # 四个卷积单元\n",
        "        # x = self.layer2(x)\n",
        "        # x = self.layer3(x)\n",
        "        # x = self.layer4(x)\n",
        "\n",
        "        # x = self.head(x)\n",
        "\n",
        "        # return x\n",
        "        \n",
        "        # RGB 分支\n",
        "        x_rgb = self.prepare_rgb(x_rgb)           # 预处理\n",
        "        x_rgb = self.layer1_rgb(x_rgb)            # 四个卷积单元\n",
        "        x_rgb = self.layer2_rgb(x_rgb)\n",
        "        x_rgb = self.layer3_rgb(x_rgb)\n",
        "        x_rgb = self.layer4_rgb(x_rgb)\n",
        "        x_rgb = self.flatten(self.pool(x_rgb))    # [Batch, 512]\n",
        "        \n",
        "        # 梯度分支\n",
        "        x_grad = self.prepare_grad(x_grad)        # 预处理\n",
        "        x_grad = self.layer1_grad(x_grad)         # 四个卷积单元\n",
        "        x_grad = self.layer2_grad(x_grad)\n",
        "        x_grad = self.layer3_grad(x_grad)\n",
        "        x_grad = self.layer4_grad(x_grad)\n",
        "        x_grad = self.flatten(self.pool(x_grad))  # [Batch, 512]\n",
        "\n",
        "        # 特征融合\n",
        "        x_fused = torch.cat((x_rgb, x_grad), dim=1)      # 拼接 [Batch, 1024]\n",
        "        \n",
        "        # 融合后处理\n",
        "        x_processed = self.relu(self.fc_fused(x_fused))  # [Batch, 1024]\n",
        "        x_processed = self.dropout(x_processed)\n",
        "        \n",
        "        # 最终分类\n",
        "        x_out = self.classifier(x_processed)             # [Batch, classes_num]\n",
        "\n",
        "        return x_out\n",
        "\n",
        "config = {\n",
        "    \"lr\": 1e-3,\n",
        "    \"momentum\": 0.9,\n",
        "    \"weight_decay\": 1e-4,\n",
        "}\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "net = ConvNet(classes_num=100).to(device)\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "# optimizer = torch.optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=config[\"momentum\"], weight_decay=config[\"weight_decay\"])\n",
        "# AdamW 优化器\n",
        "optimizer = torch.optim.AdamW(net.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
        "# 学习率调度器\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=15) # T_max = num_epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYKk56zLCxbk"
      },
      "source": [
        "# Step 4: Train the network and save model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAfQYydqC_R7",
        "outputId": "047cbac2-da53-48ac-92e7-8a72a86decfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch[1]:[001/391] Time:0.3667 Data:0.2127  loss:4.6642(4.6642)  prec@1:0.00(0.00)  \n",
            "Epoch[1]:[051/391] Time:0.1934 Data:0.1634  loss:4.1062(4.3201)  prec@1:6.25(8.36)  \n",
            "Epoch[1]:[101/391] Time:0.1908 Data:0.1638  loss:3.9402(4.1621)  prec@1:14.84(8.86)  \n",
            "Epoch[1]:[151/391] Time:0.1893 Data:0.1583  loss:3.7253(4.0639)  prec@1:13.28(9.03)  \n",
            "Epoch[1]:[201/391] Time:0.1876 Data:0.1576  loss:3.6535(3.9906)  prec@1:8.59(9.38)  \n",
            "Epoch[1]:[251/391] Time:0.1821 Data:0.1550  loss:3.7925(3.9354)  prec@1:9.38(9.62)  \n",
            "Epoch[1]:[301/391] Time:0.1790 Data:0.1540  loss:3.3561(3.8820)  prec@1:15.62(10.02)  \n",
            "Epoch[1]:[351/391] Time:0.1843 Data:0.1573  loss:3.5275(3.8291)  prec@1:12.50(10.53)  \n",
            "Epoch time: 75s\n",
            "Saving models......\n",
            "Epoch[2]:[001/391] Time:0.1885 Data:0.1634  loss:3.2908(3.2908)  prec@1:14.06(14.06)  \n",
            "Epoch[2]:[051/391] Time:0.1880 Data:0.1610  loss:3.4291(3.3852)  prec@1:14.06(15.18)  \n",
            "Epoch[2]:[101/391] Time:0.1816 Data:0.1536  loss:3.1016(3.3593)  prec@1:18.75(15.77)  \n",
            "Epoch[2]:[151/391] Time:0.1886 Data:0.1610  loss:3.1111(3.3315)  prec@1:15.62(16.29)  \n",
            "Epoch[2]:[201/391] Time:0.1861 Data:0.1574  loss:3.3118(3.3060)  prec@1:15.62(16.85)  \n",
            "Epoch[2]:[251/391] Time:0.1799 Data:0.1549  loss:3.4191(3.2742)  prec@1:14.06(17.40)  \n",
            "Epoch[2]:[301/391] Time:0.1797 Data:0.1547  loss:3.0012(3.2449)  prec@1:23.44(18.04)  \n",
            "Epoch[2]:[351/391] Time:0.1820 Data:0.1550  loss:2.8682(3.2159)  prec@1:25.78(18.45)  \n",
            "Epoch time: 71s\n",
            "Saving models......\n",
            "Epoch[3]:[001/391] Time:0.1902 Data:0.1622  loss:3.1667(3.1667)  prec@1:17.19(17.19)  \n",
            "Epoch[3]:[051/391] Time:0.1809 Data:0.1558  loss:2.7285(2.9310)  prec@1:28.12(24.31)  \n",
            "Epoch[3]:[101/391] Time:0.1760 Data:0.1493  loss:2.7286(2.8995)  prec@1:25.78(24.69)  \n",
            "Epoch[3]:[151/391] Time:0.1910 Data:0.1639  loss:2.8324(2.8812)  prec@1:21.09(24.94)  \n",
            "Epoch[3]:[201/391] Time:0.1923 Data:0.1598  loss:3.0236(2.8704)  prec@1:25.00(25.16)  \n",
            "Epoch[3]:[251/391] Time:0.1829 Data:0.1539  loss:2.7378(2.8584)  prec@1:24.22(25.40)  \n",
            "Epoch[3]:[301/391] Time:0.1876 Data:0.1566  loss:2.4998(2.8403)  prec@1:25.00(25.72)  \n",
            "Epoch[3]:[351/391] Time:0.1820 Data:0.1559  loss:2.7061(2.8283)  prec@1:29.69(25.95)  \n",
            "Epoch time: 72s\n",
            "Saving models......\n",
            "Epoch[4]:[001/391] Time:0.1842 Data:0.1572  loss:2.7880(2.7880)  prec@1:24.22(24.22)  \n",
            "Epoch[4]:[051/391] Time:0.1877 Data:0.1613  loss:2.5075(2.5702)  prec@1:33.59(31.00)  \n",
            "Epoch[4]:[101/391] Time:0.1815 Data:0.1555  loss:2.5654(2.6084)  prec@1:33.59(30.86)  \n",
            "Epoch[4]:[151/391] Time:0.1905 Data:0.1645  loss:2.6208(2.6076)  prec@1:27.34(30.98)  \n",
            "Epoch[4]:[201/391] Time:0.1883 Data:0.1552  loss:2.6599(2.5950)  prec@1:40.62(31.25)  \n",
            "Epoch[4]:[251/391] Time:0.1860 Data:0.1600  loss:2.6120(2.5840)  prec@1:23.44(31.53)  \n",
            "Epoch[4]:[301/391] Time:0.1872 Data:0.1562  loss:2.5138(2.5792)  prec@1:27.34(31.53)  \n",
            "Epoch[4]:[351/391] Time:0.1779 Data:0.1509  loss:2.5165(2.5704)  prec@1:25.00(31.75)  \n",
            "Epoch time: 73s\n",
            "Saving models......\n",
            "Epoch[5]:[001/391] Time:0.1866 Data:0.1626  loss:2.3796(2.3796)  prec@1:41.41(41.41)  \n",
            "Epoch[5]:[051/391] Time:0.1769 Data:0.1500  loss:2.5384(2.3858)  prec@1:33.59(35.51)  \n",
            "Epoch[5]:[101/391] Time:0.1989 Data:0.1679  loss:2.3894(2.3930)  prec@1:32.81(35.36)  \n",
            "Epoch[5]:[151/391] Time:0.1825 Data:0.1544  loss:2.2565(2.3941)  prec@1:32.03(35.38)  \n",
            "Epoch[5]:[201/391] Time:0.1835 Data:0.1565  loss:2.2395(2.3962)  prec@1:40.62(35.25)  \n",
            "Epoch[5]:[251/391] Time:0.1844 Data:0.1554  loss:2.3251(2.3905)  prec@1:33.59(35.48)  \n",
            "Epoch[5]:[301/391] Time:0.1921 Data:0.1650  loss:2.2677(2.3907)  prec@1:39.06(35.47)  \n",
            "Epoch[5]:[351/391] Time:0.1810 Data:0.1550  loss:2.3389(2.3863)  prec@1:35.94(35.54)  \n",
            "Epoch time: 72s\n",
            "Saving models......\n",
            "Epoch[6]:[001/391] Time:0.1880 Data:0.1619  loss:2.1380(2.1380)  prec@1:39.84(39.84)  \n",
            "Epoch[6]:[051/391] Time:0.1800 Data:0.1540  loss:2.1590(2.2380)  prec@1:41.41(38.86)  \n",
            "Epoch[6]:[101/391] Time:0.1881 Data:0.1581  loss:2.2176(2.2407)  prec@1:35.94(38.83)  \n",
            "Epoch[6]:[151/391] Time:0.1834 Data:0.1565  loss:2.0963(2.2335)  prec@1:43.75(38.72)  \n",
            "Epoch[6]:[201/391] Time:0.1880 Data:0.1580  loss:2.4738(2.2318)  prec@1:34.38(38.94)  \n",
            "Epoch[6]:[251/391] Time:0.1872 Data:0.1612  loss:2.2547(2.2408)  prec@1:42.97(38.76)  \n",
            "Epoch[6]:[301/391] Time:0.2061 Data:0.1740  loss:2.1083(2.2331)  prec@1:44.53(38.96)  \n",
            "Epoch[6]:[351/391] Time:0.1890 Data:0.1590  loss:2.2351(2.2282)  prec@1:38.28(39.14)  \n",
            "Epoch time: 74s\n",
            "Saving models......\n",
            "Epoch[7]:[001/391] Time:0.1839 Data:0.1539  loss:2.3140(2.3140)  prec@1:35.94(35.94)  \n",
            "Epoch[7]:[051/391] Time:0.1903 Data:0.1643  loss:2.2442(2.1181)  prec@1:37.50(41.65)  \n",
            "Epoch[7]:[101/391] Time:0.1884 Data:0.1584  loss:2.1648(2.0950)  prec@1:42.19(42.47)  \n",
            "Epoch[7]:[151/391] Time:0.1905 Data:0.1609  loss:2.1096(2.0955)  prec@1:38.28(42.22)  \n",
            "Epoch[7]:[201/391] Time:0.1983 Data:0.1670  loss:2.0146(2.1020)  prec@1:39.06(42.13)  \n",
            "Epoch[7]:[251/391] Time:0.1946 Data:0.1626  loss:1.9578(2.0880)  prec@1:50.00(42.39)  \n",
            "Epoch[7]:[301/391] Time:0.2013 Data:0.1703  loss:2.3309(2.0875)  prec@1:37.50(42.43)  \n",
            "Epoch[7]:[351/391] Time:0.1785 Data:0.1506  loss:2.3071(2.0946)  prec@1:38.28(42.32)  \n",
            "Epoch time: 73s\n",
            "Saving models......\n",
            "Epoch[8]:[001/391] Time:0.1803 Data:0.1543  loss:1.8791(1.8791)  prec@1:49.22(49.22)  \n",
            "Epoch[8]:[051/391] Time:0.1918 Data:0.1648  loss:1.7824(1.9426)  prec@1:54.69(45.99)  \n",
            "Epoch[8]:[101/391] Time:0.1775 Data:0.1513  loss:2.1102(1.9483)  prec@1:46.88(45.85)  \n",
            "Epoch[8]:[151/391] Time:0.1821 Data:0.1551  loss:2.0304(1.9540)  prec@1:44.53(45.41)  \n",
            "Epoch[8]:[201/391] Time:0.2048 Data:0.1748  loss:1.9939(1.9529)  prec@1:42.97(45.39)  \n",
            "Epoch[8]:[251/391] Time:0.1848 Data:0.1548  loss:2.3444(1.9480)  prec@1:40.62(45.57)  \n",
            "Epoch[8]:[301/391] Time:0.1858 Data:0.1578  loss:2.1483(1.9515)  prec@1:40.62(45.59)  \n",
            "Epoch[8]:[351/391] Time:0.1925 Data:0.1615  loss:1.8904(1.9550)  prec@1:46.88(45.56)  \n",
            "Epoch time: 73s\n",
            "Saving models......\n",
            "Epoch[9]:[001/391] Time:0.1835 Data:0.1564  loss:1.9924(1.9924)  prec@1:49.22(49.22)  \n",
            "Epoch[9]:[051/391] Time:0.1796 Data:0.1516  loss:1.8753(1.8625)  prec@1:57.81(48.45)  \n",
            "Epoch[9]:[101/391] Time:0.1898 Data:0.1617  loss:1.8561(1.8431)  prec@1:52.34(48.51)  \n",
            "Epoch[9]:[151/391] Time:0.1803 Data:0.1543  loss:1.7191(1.8336)  prec@1:45.31(48.39)  \n",
            "Epoch[9]:[201/391] Time:0.1875 Data:0.1600  loss:1.8691(1.8367)  prec@1:50.78(48.34)  \n",
            "Epoch[9]:[251/391] Time:0.2298 Data:0.2018  loss:1.8190(1.8283)  prec@1:48.44(48.60)  \n",
            "Epoch[9]:[301/391] Time:0.1803 Data:0.1543  loss:1.7551(1.8292)  prec@1:56.25(48.51)  \n",
            "Epoch[9]:[351/391] Time:0.1793 Data:0.1513  loss:1.8618(1.8328)  prec@1:47.66(48.45)  \n",
            "Epoch time: 72s\n",
            "Saving models......\n",
            "Epoch[10]:[001/391] Time:0.1936 Data:0.1626  loss:1.6399(1.6399)  prec@1:56.25(56.25)  \n",
            "Epoch[10]:[051/391] Time:0.1871 Data:0.1580  loss:1.6986(1.7207)  prec@1:54.69(51.30)  \n",
            "Epoch[10]:[101/391] Time:0.1891 Data:0.1566  loss:1.6501(1.7274)  prec@1:52.34(51.04)  \n",
            "Epoch[10]:[151/391] Time:0.1879 Data:0.1609  loss:1.5025(1.7090)  prec@1:59.38(51.27)  \n",
            "Epoch[10]:[201/391] Time:0.1818 Data:0.1534  loss:1.6904(1.7144)  prec@1:53.91(51.26)  \n",
            "Epoch[10]:[251/391] Time:0.1849 Data:0.1559  loss:1.6706(1.7138)  prec@1:54.69(51.22)  \n",
            "Epoch[10]:[301/391] Time:0.1804 Data:0.1520  loss:1.5785(1.7054)  prec@1:50.78(51.38)  \n",
            "Epoch[10]:[351/391] Time:0.1938 Data:0.1648  loss:1.9647(1.7087)  prec@1:46.88(51.42)  \n",
            "Epoch time: 73s\n",
            "Saving models......\n",
            "Epoch[11]:[001/391] Time:0.1950 Data:0.1650  loss:1.3614(1.3614)  prec@1:59.38(59.38)  \n",
            "Epoch[11]:[051/391] Time:0.1896 Data:0.1626  loss:1.7160(1.6197)  prec@1:46.88(54.24)  \n",
            "Epoch[11]:[101/391] Time:0.1872 Data:0.1572  loss:1.5211(1.6058)  prec@1:56.25(54.35)  \n",
            "Epoch[11]:[151/391] Time:0.1834 Data:0.1514  loss:1.3885(1.6014)  prec@1:60.94(54.46)  \n",
            "Epoch[11]:[201/391] Time:0.1913 Data:0.1643  loss:1.4372(1.6080)  prec@1:56.25(54.34)  \n",
            "Epoch[11]:[251/391] Time:0.1809 Data:0.1539  loss:1.7600(1.6105)  prec@1:47.66(54.24)  \n",
            "Epoch[11]:[301/391] Time:0.1853 Data:0.1583  loss:1.6328(1.6116)  prec@1:46.09(54.09)  \n",
            "Epoch[11]:[351/391] Time:0.1878 Data:0.1598  loss:1.4060(1.6102)  prec@1:60.16(54.09)  \n",
            "Epoch time: 73s\n",
            "Saving models......\n",
            "Epoch[12]:[001/391] Time:0.1885 Data:0.1635  loss:1.4055(1.4055)  prec@1:59.38(59.38)  \n",
            "Epoch[12]:[051/391] Time:0.1873 Data:0.1613  loss:1.3446(1.5250)  prec@1:62.50(56.10)  \n",
            "Epoch[12]:[101/391] Time:0.1948 Data:0.1654  loss:1.4324(1.5031)  prec@1:54.69(56.39)  \n",
            "Epoch[12]:[151/391] Time:0.1923 Data:0.1602  loss:1.5439(1.4972)  prec@1:56.25(56.54)  \n",
            "Epoch[12]:[201/391] Time:0.1774 Data:0.1514  loss:1.7479(1.5013)  prec@1:53.12(56.52)  \n",
            "Epoch[12]:[251/391] Time:0.1861 Data:0.1561  loss:1.3418(1.4944)  prec@1:63.28(56.65)  \n",
            "Epoch[12]:[301/391] Time:0.1790 Data:0.1520  loss:1.6752(1.5008)  prec@1:53.91(56.51)  \n",
            "Epoch[12]:[351/391] Time:0.1903 Data:0.1642  loss:1.5090(1.5027)  prec@1:57.03(56.45)  \n",
            "Epoch time: 73s\n",
            "Saving models......\n",
            "Epoch[13]:[001/391] Time:0.1826 Data:0.1586  loss:1.4969(1.4969)  prec@1:60.16(60.16)  \n",
            "Epoch[13]:[051/391] Time:0.1863 Data:0.1583  loss:1.5541(1.4219)  prec@1:59.38(58.30)  \n",
            "Epoch[13]:[101/391] Time:0.1971 Data:0.1671  loss:1.4103(1.4195)  prec@1:60.16(58.56)  \n",
            "Epoch[13]:[151/391] Time:0.1848 Data:0.1574  loss:1.3136(1.4217)  prec@1:61.72(58.57)  \n",
            "Epoch[13]:[201/391] Time:0.1893 Data:0.1623  loss:1.3430(1.4186)  prec@1:60.94(58.57)  \n",
            "Epoch[13]:[251/391] Time:0.1813 Data:0.1554  loss:1.2702(1.4174)  prec@1:60.16(58.69)  \n",
            "Epoch[13]:[301/391] Time:0.1855 Data:0.1595  loss:1.5518(1.4188)  prec@1:53.91(58.73)  \n",
            "Epoch[13]:[351/391] Time:0.1910 Data:0.1635  loss:1.5717(1.4212)  prec@1:51.56(58.63)  \n",
            "Epoch time: 73s\n",
            "Saving models......\n",
            "Epoch[14]:[001/391] Time:0.1823 Data:0.1532  loss:1.4526(1.4526)  prec@1:64.06(64.06)  \n",
            "Epoch[14]:[051/391] Time:0.1788 Data:0.1528  loss:1.4870(1.3998)  prec@1:58.59(58.98)  \n",
            "Epoch[14]:[101/391] Time:0.1905 Data:0.1635  loss:1.5047(1.3749)  prec@1:53.91(59.58)  \n",
            "Epoch[14]:[151/391] Time:0.1910 Data:0.1629  loss:1.2180(1.3723)  prec@1:58.59(59.53)  \n",
            "Epoch[14]:[201/391] Time:0.1841 Data:0.1561  loss:1.4207(1.3724)  prec@1:61.72(59.67)  \n",
            "Epoch[14]:[251/391] Time:0.1799 Data:0.1519  loss:1.4574(1.3777)  prec@1:57.03(59.54)  \n",
            "Epoch[14]:[301/391] Time:0.1943 Data:0.1673  loss:1.3339(1.3804)  prec@1:60.94(59.54)  \n",
            "Epoch[14]:[351/391] Time:0.1837 Data:0.1577  loss:1.2465(1.3730)  prec@1:61.72(59.79)  \n",
            "Epoch time: 74s\n",
            "Saving models......\n",
            "Epoch[15]:[001/391] Time:0.1825 Data:0.1575  loss:1.2588(1.2588)  prec@1:57.81(57.81)  \n",
            "Epoch[15]:[051/391] Time:0.1976 Data:0.1656  loss:1.4333(1.3250)  prec@1:60.16(60.66)  \n",
            "Epoch[15]:[101/391] Time:0.1978 Data:0.1688  loss:1.5009(1.3217)  prec@1:56.25(61.14)  \n",
            "Epoch[15]:[151/391] Time:0.1949 Data:0.1669  loss:1.3386(1.3327)  prec@1:60.16(60.65)  \n",
            "Epoch[15]:[201/391] Time:0.1911 Data:0.1611  loss:1.3719(1.3340)  prec@1:61.72(60.78)  \n",
            "Epoch[15]:[251/391] Time:0.1841 Data:0.1571  loss:1.6496(1.3335)  prec@1:58.59(60.87)  \n",
            "Epoch[15]:[301/391] Time:0.1810 Data:0.1539  loss:1.2454(1.3368)  prec@1:64.84(60.83)  \n",
            "Epoch[15]:[351/391] Time:0.1855 Data:0.1585  loss:1.2206(1.3409)  prec@1:60.16(60.80)  \n",
            "Epoch time: 73s\n",
            "Saving models......\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "class AverageMeter(object):\n",
        "    def __init__(self):\n",
        "      self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "      self.val   = 0\n",
        "      self.avg   = 0\n",
        "      self.sum   = 0\n",
        "      self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "      self.val   = val\n",
        "      self.sum   += val * n\n",
        "      self.count += n\n",
        "      self.avg   = self.sum / self.count\n",
        "\n",
        "def accuracy(output, target, topk=(1,1)):\n",
        "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred    = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "      correct_k = correct[:k].view(-1).float().sum(0)\n",
        "      res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res[0]\n",
        "\n",
        "def train(train_loader, net, optimizer, criterion, epoch):\n",
        "    batch_time = AverageMeter()\n",
        "    data_time  = AverageMeter()\n",
        "    top1       = AverageMeter()\n",
        "\n",
        "    LOSS = AverageMeter()\n",
        "\n",
        "    net.train()\n",
        "\n",
        "    end = time.time()\n",
        "    for i, (img_rgb, img_grad, target) in enumerate(train_loader, start=1):\n",
        "        data_time.update(time.time() - end)\n",
        "\n",
        "        img_rgb = img_rgb.to(device)\n",
        "        img_grad = img_grad.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        out = net(img_rgb, img_grad)\n",
        "\n",
        "        loss = criterion(out, target)\n",
        "\n",
        "        prec1 = accuracy(out, target) # prec1:list\n",
        "        top1.update(prec1.item(), img_rgb.size(0))\n",
        "\n",
        "        LOSS.update(loss.item(), img_rgb.size(0))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        if i % 50 == 1:\n",
        "            log_str = ('Epoch[{0}]:[{1:03}/{2:03}] '\n",
        "                       'Time:{batch_time.val:.4f} '\n",
        "                       'Data:{data_time.val:.4f}  '\n",
        "                       'loss:{loss.val:.4f}({loss.avg:.4f})  '\n",
        "                       'prec@1:{top1.val:.2f}({top1.avg:.2f})  '.format(\n",
        "                       epoch, i, len(train_loader), batch_time=batch_time, data_time=data_time,\n",
        "                       loss=LOSS,\n",
        "                       top1=top1))\n",
        "            print(log_str)\n",
        "\n",
        "    return LOSS.avg\n",
        "\n",
        "def save_checkpoint(state, is_best, save_root, epoch):\n",
        "    if not os.path.exists(save_root):\n",
        "        os.makedirs(save_root)\n",
        "    save_path = os.path.join(save_root, 'epoch_{}.pth.tar'.format(str(epoch)))\n",
        "    torch.save(state, save_path)\n",
        "    if is_best:\n",
        "        best_save_path = os.path.join(save_root, 'model_best.pth.tar')\n",
        "        shutil.copyfile(save_path, best_save_path)\n",
        "\n",
        "config = {\n",
        "    \"save_root\": \"./result\",\n",
        "    \"epochs\": 15,\n",
        "}\n",
        "\n",
        "best_top1 = 0\n",
        "test_top1 = 0\n",
        "for epoch in range(1, config[\"epochs\"]+1):\n",
        "    # train one epoch\n",
        "    epoch_start_time = time.time()\n",
        "    train_loss = train(train_loader, net, optimizer, criterion, epoch)\n",
        "\n",
        "    if 'scheduler' in locals(): # 检查 scheduler 是否已定义\n",
        "        scheduler.step()\n",
        "\n",
        "    # evaluate on testing set\n",
        "    # test_top1 = test(test_loader, net, criterion)\n",
        "\n",
        "    epoch_duration = time.time() - epoch_start_time\n",
        "    print('Epoch time: {}s'.format(int(epoch_duration)))\n",
        "\n",
        "    # save model\n",
        "    is_best = False\n",
        "    # if test_top1 > best_top1:\n",
        "    #     best_top1 = test_top1\n",
        "    #     is_best = True\n",
        "    print('Saving models......')\n",
        "    save_checkpoint({\n",
        "        'epoch': epoch,\n",
        "        'net': net.state_dict(),\n",
        "        'prec@1': test_top1,\n",
        "    }, is_best, config[\"save_root\"], epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WO1MiuPHFpVW"
      },
      "source": [
        "# Step 5: Test on single image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFsBN0Z0FxkT",
        "outputId": "c498d1e4-0e97-4e91-8317-a695211475c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bowl\n"
          ]
        }
      ],
      "source": [
        "img = Image.open(\"./cifar100/test/apple_9904.jpg\")\n",
        "# img = transform_test(img).unsqueeze(0).to(device)\n",
        "img_rgb = transform_test(img)\n",
        "img_grad = test_dataset.RGB2Gradient(img_rgb)\n",
        "\n",
        "img_rgb = img_rgb.unsqueeze(0).to(device)\n",
        "img_grad = img_grad.unsqueeze(0).to(device)\n",
        "\n",
        "out = net(img_rgb, img_grad)\n",
        "predicted_classes = torch.argmax(out, dim=1)\n",
        "tags = ['apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', 'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', 'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion', 'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse', 'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear', 'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine', 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', 'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout', 'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm']\n",
        "\n",
        "print(tags[predicted_classes[0]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ne7evEhmEI0t"
      },
      "source": [
        "# Step 6: Evaluate model accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgH66ll9EV0p",
        "outputId": "d9579d0f-155a-4068-ba05-5c4b0e6e97ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------test classification result---------------------------------\n",
            "Loss: 3.6744, Prec@1: 47.33%\n"
          ]
        }
      ],
      "source": [
        "def test(test_loader, net, criterion):\n",
        "    losses = AverageMeter()\n",
        "    top1   = AverageMeter()\n",
        "\n",
        "    net.eval()\n",
        "\n",
        "    for i, (img_rgb, img_grad, target) in enumerate(test_loader, start=1):\n",
        "        img_rgb = img_rgb.to(device)\n",
        "        img_grad = img_grad.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out = net(img_rgb, img_grad)\n",
        "            loss = criterion(out, target)\n",
        "\n",
        "        prec1 = accuracy(out, target) # prec1:list\n",
        "        losses.update(loss.item(), img_rgb.size(0))\n",
        "        top1.update(prec1.item(), img_rgb.size(0))\n",
        "\n",
        "    f_l = [losses.avg, top1.avg]\n",
        "    print('---------------------------------test classification result---------------------------------')\n",
        "    print('Loss: {:.4f}, Prec@1: {:.2f}%'.format(*f_l))\n",
        "\n",
        "    return top1.avg\n",
        "\n",
        "test_top1 = test(test_loader, net, criterion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0v4gr3XWJDBQ"
      },
      "source": [
        "# Step 7: T-SNE Visualization\n",
        "\n",
        "## Use hooks in PyTorch to extract feature representations from the intermediate layers of the model for the test set \"testloader\", and visualize them using the T-SNE method. The specific requirements are as follows:\n",
        "#### Visualize the features before and after the dual-branch feature fusion. If there are multiple fusions, you may choose specific layers for visualization.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pytorch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
